{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook from : https://www.kaggle.com/code/jiaowoguanren/sars-cov-2-ct-scan-dataset-classification-tf-0-99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import pathlib, splitfolders\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import  ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OF SARSCOV2-CTSCAN-DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "img_height, img_width = 300, 300\n",
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "def create_data_binary(data_bs):\n",
    "    data_bs = pathlib.Path(data_bs)\n",
    "    splitfolders.ratio(data_bs, output='../sarscov2-ctscan-dataset-splitted/', seed=1234, ratio=(0.7, 0.15, 0.15), group_prefix=None)\n",
    "    data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    train_ds = data_gen.flow_from_directory('../sarscov2-ctscan-dataset-splitted/train/', target_size=(img_height, img_width),\n",
    "                                            class_mode='binary', batch_size=batch_size, subset='training')\n",
    "    val_ds = data_gen.flow_from_directory('../sarscov2-ctscan-dataset-splitted/val/', target_size=(img_height, img_width),\n",
    "                                          class_mode='binary', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "train_data, val_data = create_data_binary('../sarscov2-ctscan-dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OF COVID-CT-master DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "img_height, img_width = 150, 150\n",
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "data_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "\"\"\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                  zoom_range=0.2,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  fill_mode='nearest')\n",
    "\"\"\"\n",
    "                                  \n",
    "train_data = data_gen.flow_from_directory(\"../COVID-CT-master/Dataset/train/\",\n",
    "                                          target_size=(img_height, img_width),\n",
    "                                          batch_size = batch_size,\n",
    "                                          class_mode = 'binary',\n",
    "                                          subset='training')\n",
    "                                         \n",
    "test_data = data_gen.flow_from_directory(\"../COVID-CT-master/Dataset/test/\",\n",
    "                                          target_size=(img_height, img_width),\n",
    "                                          batch_size = batch_size,\n",
    "                                          class_mode = 'binary')\n",
    "\n",
    "val_data = data_gen.flow_from_directory(\"../COVID-CT-master/Dataset/val/\",\n",
    "                                          target_size=(img_height, img_width),\n",
    "                                          batch_size = batch_size,\n",
    "                                          class_mode = 'binary',\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "        self.C1 = Conv2D(32, (3 * 3), padding='same', input_shape = input_shape)\n",
    "        self.B1 = BatchNormalization()\n",
    "        self.A1 = Activation('relu')\n",
    "        self.P1 = MaxPooling2D(2, padding='same')\n",
    "        \n",
    "        self.C2 = Conv2D(32, (3 * 3), padding='same')\n",
    "        self.B2 = BatchNormalization()\n",
    "        self.A2 = Activation('relu')\n",
    "        self.P2 = MaxPooling2D(2, padding='same')\n",
    "        self.Dr1 = Dropout(0.3)\n",
    "        \n",
    "        self.C3 = Conv2D(32, (3 * 3), padding='same')\n",
    "        self.B3 = BatchNormalization()\n",
    "        self.A3 = Activation('relu')\n",
    "        self.P3 = MaxPooling2D(2, padding='same')\n",
    "        self.Dr2 = Dropout(0.3)\n",
    "        \n",
    "        self.F1 = Flatten()\n",
    "        self.D1 = Dense(256, activation='relu')\n",
    "        self.B4 = BatchNormalization()\n",
    "        self.D2 = Dense(256, activation='relu')\n",
    "        self.D3 = Dense(256, activation='relu')\n",
    "        self.D4 = Dense(256, activation='relu')\n",
    "        self.Dr3 = Dropout(0.3)\n",
    "        self.D5 = Dense(1, activation='sigmoid')\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.B1(x)\n",
    "        x = self.A1(x)\n",
    "        x = self.P1(x)\n",
    "        \n",
    "        x = self.C2(x)\n",
    "        x = self.B2(x)\n",
    "        x = self.A2(x)\n",
    "        x = self.P2(x)\n",
    "        x = self.Dr1(x)\n",
    "        \n",
    "        x = self.C3(x)\n",
    "        x = self.B3(x)\n",
    "        x = self.A3(x)\n",
    "        x = self.P3(x)\n",
    "        x = self.Dr2(x)\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.D1(x)\n",
    "        x = self.B4(x)\n",
    "        x = self.D2(x)\n",
    "        x = self.D3(x)\n",
    "        x = self.D4(x)\n",
    "        x = self.Dr3(x)\n",
    "        y = self.D5(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        name = 'Huang_Model'\n",
    "        return name\n",
    "    \n",
    "    \n",
    "net = BaseModel()\n",
    "\n",
    "net.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "checkpoint_save_path = './Model.ckpt'\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    net.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True,\n",
    "                                                 save_best_only=True)\n",
    "\n",
    "#history = net.fit(train_data, epochs=30, batch_size=batch_size, callbacks=[cp_callback])\n",
    "history = net.fit(train_data, epochs=30, batch_size=batch_size)\n",
    "\n",
    "net.summary()\n",
    "\n",
    "file = open('./weights.txt', 'w')\n",
    "for v in net.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types = ['COVID', 'non-COVID']\n",
    "\n",
    "data_dir = '../sarscov2-ctscan-dataset-splitted/'\n",
    "\n",
    "# resize images from data_dir train, test and val and create new dataset resized\n",
    "def resize_images(data_dir):\n",
    "    # resize images from data_dir train, test and val and create new dataset resized\n",
    "    for folder in ['train', 'test', 'val']:\n",
    "        for file in os.listdir(os.path.join(data_dir, folder)):\n",
    "            if file.endswith('.jpg'):\n",
    "\n",
    "                img = cv2.imread(os.path.join(data_dir, folder, file))\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                cv2.imwrite(os.path.join(data_dir, 'resized', folder, file), img)\n",
    "\n",
    "# resize_images(data_dir)\n",
    "\n",
    "# create data generator for train, test and val\n",
    "def create_data_binary(data_dir):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    # create data generator for train, test and val\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        os.path.join(data_dir, 'train'),\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        label_mode='binary')\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        os.path.join(data_dir, 'test'),\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        label_mode='binary')\n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = create_data_binary('../sarscov2-ctscan-dataset-splitted/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Model, self).__init__()\n",
    "        \n",
    "        self.C1 = Conv2D(32, (3 * 3), padding='same', input_shape = input_shape)\n",
    "\n",
    "        #self.VGG = VGG16(weights='imagenet', include_top = False, input_shape= input_shape)\n",
    "        self.VGG = VGG16(weights='imagenet', include_top = False)\n",
    "\n",
    "        self.GAP = GlobalAveragePooling2D()\n",
    "        self.B1 = BatchNormalization()\n",
    "\n",
    "        self.D1 = Dense(32, activation='relu')\n",
    "        self.B2 = BatchNormalization()\n",
    "        \n",
    "        self.D2 = Dense(32, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.VGG(x)\n",
    "        x = self.GAP(x)\n",
    "        x = self.B1(x)\n",
    "        x = self.D1(x)\n",
    "        x = self.B2(x)\n",
    "        y = self.D2(x)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def __repr__(self):\n",
    "        name = 'VGG16Model'\n",
    "        return name\n",
    "        \n",
    "net = VGG16Model()\n",
    "\n",
    "optimizer = Adam(learning_rate= 0.003, beta_1 = 0.9, beta_2 = 0.999, epsilon = 0.1, decay = 0.0)\n",
    "net.compile(loss = 'categorical_crossentropy', optimizer =optimizer, metrics = ['accuracy'])\n",
    "\n",
    "annealer = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.70, patience = 5, verbose = 1, min_lr = 1e-4)\n",
    "\n",
    "# Fits the model on batches with real-time data augmentation\n",
    "#history = net.fit(X_Train, epochs=epochs, batch_size = batch_size, callbacks = [annealer])\n",
    "history = net.fit(train_ds, epochs=epochs, batch_size = batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc)\n",
    "plt.title('Training Acc')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss)\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete new structure VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_types = ['COVID', 'non-COVID']\n",
    "\n",
    "train_dir = data_dir = '../sarscov2-ctscan-dataset'\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for index, sp in enumerate(disease_types):\n",
    "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
    "        train_data.append([sp + \"/\" + file, index, sp])\n",
    "        \n",
    "train = pd.DataFrame(train_data, columns = ['File', 'ID','Disease Type'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seed = 7\n",
    "IMAGE_SIZE = 224\n",
    "train = train.sample(frac = 1, replace=False, random_state = Seed) \n",
    "train = train.reset_index(drop = True) # Reset indices (row numbers)\n",
    "\n",
    "sns.countplot(x = \"ID\", data = train).set_title(\"Frequency Histogram of Disease IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_defects(defect_types, rows, cols):\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "    defect_files = train['File'][train['Disease Type'] == defect_types].values\n",
    "    \n",
    "    n = 0\n",
    "    fig.suptitle(defect_types, fontsize = 22, color = \"white\")\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            image_path = os.path.join(data_dir, defect_files[n])\n",
    "            ax[i, j].set_xticks([])\n",
    "            ax[i, j].set_yticks([])\n",
    "            ax[i, j].imshow(cv2.imread(image_path))\n",
    "            n += 1\n",
    "            \n",
    "# Displays first n images of class from training set\n",
    "plot_defects('COVID', 2, 2)\n",
    "plot_defects('non-COVID', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filepath):\n",
    "    return cv2.imread(os.path.join(data_dir, filepath)) # Loading a color image is the default flag\n",
    "\n",
    "# Resize image to target size\n",
    "def resize_image(image, image_size):\n",
    "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "for i, file in enumerate(train['File'].values):\n",
    "    image = read_image(file)\n",
    "    if image is not None:\n",
    "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "# Normalize the data\n",
    "X_Train = X_train / 255.0\n",
    "print('Train Shape:', X_Train.shape)\n",
    "\n",
    "Y_train = to_categorical(train['ID'].values, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and validation sets \n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_Train,\n",
    "                                                  Y_train,\n",
    "                                                  test_size = 0.2,\n",
    "                                                  random_state = Seed)\n",
    "\n",
    "print(f'X_train:',X_train.shape)\n",
    "print(f'X_val:',X_val.shape)\n",
    "print(f'Y_train:',Y_train.shape)\n",
    "print(f'Y_val:',Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg(IMAGE_SIZE, channels):\n",
    "    \n",
    "    VGG = VGG16(weights='imagenet', include_top = False)\n",
    "    \n",
    "    input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, channels))\n",
    "    x = Conv2D(3, (3, 3), padding='same')(input)\n",
    "    \n",
    "    x = VGG(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    output = Dense(2,activation = 'softmax')(x)\n",
    " \n",
    "\n",
    "    # model\n",
    "    model = Model(input,output)\n",
    "   \n",
    "    \n",
    "    optimizer = Adam(learning_rate= 0.003, beta_1 = 0.9, beta_2 = 0.999, epsilon = 0.1, decay = 0.0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer =optimizer, metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "\n",
    "model = build_vgg(IMAGE_SIZE, channels)\n",
    "annealer = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.70, patience = 5, verbose = 1, min_lr = 1e-4)\n",
    "# Generates batches of image data with data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range = 360, # Degree range for random rotations\n",
    "                        width_shift_range = 0.2, # Range for random horizontal shifts\n",
    "                        height_shift_range = 0.2, # Range for random vertical shifts\n",
    "                        zoom_range = 0.2, # Range for random zoom\n",
    "                        horizontal_flip = True, # Randomly flip inputs horizontally\n",
    "                        vertical_flip = True) # Randomly flip inputs vertically\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file = 'convnet.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 65\n",
    "\n",
    "\n",
    "# Fits the model on batches with real-time data augmentation\n",
    "hist = model.fit(datagen.flow(X_train, Y_train, batch_size = BATCH_SIZE),\n",
    "               steps_per_epoch = X_train.shape[0] // BATCH_SIZE,\n",
    "               epochs = EPOCHS,\n",
    "               verbose = 1,\n",
    "               callbacks = [annealer],\n",
    "               validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_accuracy = model.evaluate(X_val, Y_val)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "Y_true = np.argmax(Y_val, axis=1)\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(cm, cmap=plt.cm.Oranges, annot=True, square=True, xticklabels=disease_types, yticklabels=disease_types)\n",
    "ax.set_ylabel('Actual', fontsize=40)\n",
    "ax.set_xlabel('Predicted', fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy plot \n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# loss plot\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
