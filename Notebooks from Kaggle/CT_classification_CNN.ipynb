{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook from : https://www.kaggle.com/code/pulavendranselvaraj/covid-noncovid-ct-classification-using-cnn-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PILLOW_VERSION' from 'PIL' (/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/PIL/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader,Dataset\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchinfo\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/datasets/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msvhn\u001b[39;00m \u001b[39mimport\u001b[39;00m SVHN\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mphototour\u001b[39;00m \u001b[39mimport\u001b[39;00m PhotoTour\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfakedata\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeData\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msemeion\u001b[39;00m \u001b[39mimport\u001b[39;00m SEMEION\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39momniglot\u001b[39;00m \u001b[39mimport\u001b[39;00m Omniglot\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/datasets/fakedata.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdata\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFakeData\u001b[39;00m(data\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A fake dataset that returns randomly generated images and returns them as PIL images\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m     18\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mCompose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mToTensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mToPILImage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNormalize\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mResize\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mScale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCenterCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPad\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mLambda\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomApply\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomChoice\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomOrder\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomHorizontalFlip\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mRandomVerticalFlip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomResizedCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomSizedCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFiveCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTenCrop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLinearTransformation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mColorJitter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomRotation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomAffine\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGrayscale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRandomGrayscale\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m _pil_interpolation_to_str \u001b[39m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     Image\u001b[39m.\u001b[39mNEAREST: \u001b[39m'\u001b[39m\u001b[39mPIL.Image.NEAREST\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     Image\u001b[39m.\u001b[39mBILINEAR: \u001b[39m'\u001b[39m\u001b[39mPIL.Image.BILINEAR\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     Image\u001b[39m.\u001b[39mBICUBIC: \u001b[39m'\u001b[39m\u001b[39mPIL.Image.BICUBIC\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     Image\u001b[39m.\u001b[39mLANCZOS: \u001b[39m'\u001b[39m\u001b[39mPIL.Image.LANCZOS\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m }\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/functional.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageOps, ImageEnhance, PILLOW_VERSION\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39maccimage\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PILLOW_VERSION' from 'PIL' (/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.11/site-packages/PIL/__init__.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path=\"/kaggle/input/sarscov2-ctscan-dataset/\"\n",
    "class_labels=os.listdir(input_path)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths=glob.glob(input_path+\"**/*.*\")\n",
    "print(filepaths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['filepaths']=filepaths\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filepaths'][0].split(\"/\")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing split operation on each filepath to collect class labels\n",
    "df['labels']=df['filepaths'].apply(lambda x: x.split(\"/\")[4])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={}\n",
    "for idx,label in enumerate(df['labels'].unique().tolist()):\n",
    "    class_dict[label]=idx\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels']=df['labels'].map(class_dict)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['filepaths'].values,df['labels'].values,test_size=0.2,random_state=42,shuffle=True)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class C_NC_Dataset(Dataset):\n",
    "    def __init__(self,image_paths,image_labels,image_transforms=None,grayscale=True):\n",
    "        self.image_paths=image_paths\n",
    "        self.image_labels=torch.Tensor(image_labels)\n",
    "        \n",
    "        if (image_transforms==None) and (grayscale==True):\n",
    "            self.transform=transforms.Compose([transforms.Grayscale(),\n",
    "                                                transforms.Resize(size=(250,250)),\n",
    "                                                transforms.ToTensor()])\n",
    "        elif grayscale==False:\n",
    "            self.transform=transforms.Compose([transforms.Resize(size=(250,250)),\n",
    "                                              transforms.ToTensor()])\n",
    "        else:\n",
    "            self.transform=image_transforms\n",
    "    def __getitem__(self,index):\n",
    "        current_image_path=self.image_paths[index]\n",
    "        current_image=PIL.Image.open(current_image_path).convert(mode=\"RGB\")\n",
    "        transformed_image=self.transform(current_image)\n",
    "        current_label=self.image_labels[index]\n",
    "        return transformed_image,current_label\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=C_NC_Dataset(image_paths=X_train,image_labels=y_train)\n",
    "val_dataset=C_NC_Dataset(image_paths=X_test,image_labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"number of records in train_loader: {len(train_loader.dataset)}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"number of batches in val_loader: {len(val_loader)}\")\n",
    "print(f\"number of records in val_loader: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "\n",
    "class Convnet(nn.Module):\n",
    "    def __init__(self,dropout=0.4):\n",
    "        super(Convnet,self).__init__()\n",
    "        self.convnet=nn.Sequential(\n",
    "            # input shape - (num_batch,1,250,250)\n",
    "            nn.Conv2d(in_channels=1,out_channels=64,kernel_size=3), # output shape - (num_batch,64,248,248)\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # output shape - (num_batch,64,124,124)\n",
    "            \n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3), # output shape - (num_batch,128,122,122)\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # output shape - (num_batch,128,61,61)\n",
    "            \n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3), # output shape - (num_batch,256,59,59)\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # output shape - (num_batch,256,29,29)\n",
    "            \n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3), # output shape - (num_batch,512,27,27)\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # output shape - (num_batch,512,13,13)\n",
    "            \n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3), # output shape - (num_batch,512,11,11)\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # output shape - (num_batch,512,5,5)\n",
    "            \n",
    "            nn.Flatten() # output shape - (num_batch,12800)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=12800,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512,out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128,out_features=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.convnet(x)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Convnet(dropout=0.4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,(32,1,250,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(params=model.parameters(),lr=0.0001)\n",
    "criterion=nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model,train_loader,val_loader,device,optimizer,criterion,batch_size,epochs):\n",
    "    model=model.to(device)\n",
    "    history={\"train_accuracy\":[],\"train_loss\":[],\"val_accuracy\":[],\"val_loss\":[]}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_accuracy=0\n",
    "        train_loss=0\n",
    "        val_accuracy=0\n",
    "        val_loss=0\n",
    "        \n",
    "        for X,y in train_loader:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            \n",
    "            # forward propagation\n",
    "            outputs=model(X).view(-1)\n",
    "            pred=torch.sigmoid(outputs)\n",
    "            pred=torch.round(pred)\n",
    "            \n",
    "            # loss computation\n",
    "            loss=criterion(outputs,y)\n",
    "            cur_train_loss=loss.item()\n",
    "            \n",
    "            # conducting backward propagation and updating model parameters\n",
    "            optimizer.zero_grad() # setting gradient to zero to avoid gradient accumulating\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cur_train_accuracy=(pred==y).sum().item()/batch_size\n",
    "            \n",
    "            train_accuracy+=cur_train_accuracy\n",
    "            train_loss+=cur_train_loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X,y in val_loader:\n",
    "                X=X.to(device)\n",
    "                y=y.to(device)\n",
    "                \n",
    "                outputs=model(X).view(-1)\n",
    "                pred=torch.sigmoid(outputs)\n",
    "                pred=torch.round(pred)\n",
    "                \n",
    "                loss=criterion(outputs,y)\n",
    "                cur_val_loss=loss.item()\n",
    "                cur_val_accuracy=(pred==y).sum().item()/batch_size\n",
    "                \n",
    "                val_accuracy+=cur_val_accuracy\n",
    "                val_loss+=cur_val_loss\n",
    "        train_accuracy=train_accuracy/len(train_loader)\n",
    "        train_loss=train_loss/len(train_loader)\n",
    "        val_accuracy=val_accuracy/len(val_loader)\n",
    "        val_loss=val_loss/len(val_loader)\n",
    "        \n",
    "        print(f\"[{epoch+1:>2d}/{epochs:>2d}], train_accuracy:{train_accuracy:>5f}, train_loss:{train_loss:>5f}, val_accuracy:{val_accuracy:>5f}, val_loss:{val_loss:>5f}\")\n",
    "        \n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "    PATH=\"/kaggle/working/ConvolutionalNeuralNetwork_model.pt\"\n",
    "    torch.save(model.state_dict(),PATH)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_training(model,train_loader,val_loader,device,optimizer,criterion,batch_size=32,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(style=\"ggplot\"):\n",
    "    plt.figure(figsize=(18,8))\n",
    "    plt.plot(history['train_accuracy'],label='train accuracy')\n",
    "    plt.plot(history['val_accuracy'],label='val accuracy')\n",
    "    plt.title(label='Accuracy plots')\n",
    "    plt.xlabel(xlabel='epochs')\n",
    "    plt.ylabel(ylabel='accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(18,8))\n",
    "    plt.plot(history['train_loss'],label='train loss')\n",
    "    plt.plot(history['val_loss'],label='val loss')\n",
    "    plt.title(label='Loss plots')\n",
    "    plt.xlabel(xlabel='epochs')\n",
    "    plt.ylabel(ylabel='loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
